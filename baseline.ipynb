{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7becb7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['tenure', 'monthlycharges', 'totalcharges']\n",
      "Categorical columns: ['gender', 'seniorcitizen', 'partner', 'dependents', 'phoneservice', 'multiplelines', 'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling', 'paymentmethod']\n",
      "============================================================\n",
      "Training and Evaluating: Logistic Regression\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       826\n",
      "           1       0.63      0.57      0.60       299\n",
      "\n",
      "    accuracy                           0.80      1125\n",
      "   macro avg       0.74      0.72      0.73      1125\n",
      "weighted avg       0.79      0.80      0.79      1125\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating: KNN\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       826\n",
      "           1       0.55      0.52      0.53       299\n",
      "\n",
      "    accuracy                           0.76      1125\n",
      "   macro avg       0.69      0.68      0.68      1125\n",
      "weighted avg       0.75      0.76      0.76      1125\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating: SVM\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       826\n",
      "           1       0.64      0.48      0.55       299\n",
      "\n",
      "    accuracy                           0.79      1125\n",
      "   macro avg       0.73      0.69      0.70      1125\n",
      "weighted avg       0.78      0.79      0.78      1125\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating: Decision Tree\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82       826\n",
      "           1       0.51      0.55      0.53       299\n",
      "\n",
      "    accuracy                           0.74      1125\n",
      "   macro avg       0.67      0.68      0.67      1125\n",
      "weighted avg       0.75      0.74      0.74      1125\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating: Random Forest\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.86       826\n",
      "           1       0.61      0.48      0.54       299\n",
      "\n",
      "    accuracy                           0.78      1125\n",
      "   macro avg       0.72      0.68      0.70      1125\n",
      "weighted avg       0.77      0.78      0.77      1125\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:52:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       826\n",
      "           1       0.60      0.53      0.56       299\n",
      "\n",
      "    accuracy                           0.78      1125\n",
      "   macro avg       0.72      0.70      0.71      1125\n",
      "weighted avg       0.77      0.78      0.78      1125\n",
      "\n",
      "============================================================\n",
      "Training and Evaluating: LightGBM\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3304\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265778 -> initscore=-1.016151\n",
      "[LightGBM] [Info] Start training from score -1.016151\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.87       826\n",
      "           1       0.65      0.55      0.59       299\n",
      "\n",
      "    accuracy                           0.80      1125\n",
      "   macro avg       0.75      0.72      0.73      1125\n",
      "weighted avg       0.79      0.80      0.79      1125\n",
      "\n",
      "\n",
      "Model Evaluation Summary Table:\n",
      "                     Accuracy  Precision    Recall  F1-Score\n",
      "Logistic Regression  0.797333   0.632959  0.565217  0.597173\n",
      "LightGBM             0.800000   0.646825  0.545151  0.591652\n",
      "XGBoost              0.779556   0.595506  0.531773  0.561837\n",
      "SVM                  0.789333   0.638393  0.478261  0.546845\n",
      "Random Forest        0.779556   0.608511  0.478261  0.535581\n",
      "KNN                  0.757333   0.546099  0.515050  0.530120\n",
      "Decision Tree        0.738667   0.507740  0.548495  0.527331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Baseline Model Building with Pipelines and Classification Report\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load train.csv\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=\"churn\")\n",
    "y = train[\"churn\"].map({\"No\": 0, \"Yes\": 1})  # Encode churn: No -> 0, Yes -> 1\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: {num_cols}\")\n",
    "print(f\"Categorical columns: {cat_cols}\")\n",
    "\n",
    "# Preprocessing for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Split train again into sub train-validation set for quick model testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, clf in models.items():\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training and Evaluating: {name}\")\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('classifier', clf)])\n",
    "    scores = evaluate_model(pipe, X_train, X_val, y_train, y_val)\n",
    "    results[name] = scores\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Evaluation Summary Table:\")\n",
    "print(results_df.sort_values(by=\"F1-Score\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3a61f",
   "metadata": {},
   "source": [
    " Baseline Model Building (Step-by-Step Explanation)\n",
    "\n",
    "1. Loaded the dataset \"train.csv\" and separated the features (X) and target (y).\n",
    "2. Encoded the target column \"churn\" as 0 (No) and 1 (Yes).\n",
    "3. Identified numerical and categorical columns from the features.\n",
    "4. Built separate preprocessing pipelines:\n",
    "   - For numerical columns: handled missing values and scaled features.\n",
    "   - For categorical columns: handled missing values and applied one-hot encoding.\n",
    "5. Combined both pipelines into a single ColumnTransformer for full preprocessing.\n",
    "6. Defined multiple machine learning models:\n",
    "   - Logistic Regression, KNN, SVM, Decision Tree, Random Forest, XGBoost, and LightGBM.\n",
    "7. Created a pipeline combining preprocessing and each model.\n",
    "8. Split the dataset into training and testing sets (80% train, 20% test) with stratified sampling.\n",
    "9. Trained each model using the pipeline and evaluated performance:\n",
    "   - Calculated Accuracy, Precision\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
